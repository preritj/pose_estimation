###########################################
# data parameters
###########################################
--- !data
# add datasets here:
# name, data_dir, img_dir and tfrecord_path must be specified
# annotation files must be specified if tfrecord doesn't exist
# weight defaults to 1. if not specified
# overwrite_tfrecord defaults to false if not specified
# img_shape defaults to None if not specified

# For example to include coco train and validation sets:
# datasets:
#  - name: coco
#    data_dir: /media/storage/datasets/pose/coco
#    img_dir: images/train2017
#    tfrecord_path: tfrecords/
#    annotation_files: annotations/person_keypoints_train2017.json
#  - name: coco
#    data_dir: /media/storage/datasets/pose/coco
#    img_dir: images/val2017
#    tfrecord_path: tfrecords/val2017.records
#    annotation_files: annotations/person_keypoints_val2017.json

datasets:
#  - name: coco
#    data_dir: /media/storage/datasets/pose/coco
#    img_dir: images/val2017
#    tfrecord_path: tfrecords/val2017.records
#    annotation_files: annotations/person_keypoints_val2017.json
  - name: ava
    data_dir: /media/admin-u/data/TrainData
    img_dir: .
    tfrecord_files: tfrecords/pose/ava*.records
    annotation_files: Annotations/pose/ava*.json
    weight: 1.
    overwrite_tfrecord: false


# keypoints lookup dictionary
keypoints:
  head: 0
  nose: 1
  neck: 2
  left_shoulder: 3
  right_shoulder: 4
  left_elbow: 5
  right_elbow: 6
  left_wrist: 7
  right_wrist: 8
  left_hip: 9
  right_hip: 10
  left_knee: 11
  right_knee: 12
  left_ankle: 13
  right_ankle: 14

# segments constructed using keypoints
skeleton:
  - [0, 1]
  - [1, 2]
  - [2, 3]
  - [2, 4]
  - [2, 9]
  - [2, 10]
  - [3, 5]
  - [4, 6]
  - [5, 7]
  - [6, 8]
  - [9, 11]
  - [10, 12]
  - [11, 13]
  - [12, 14]

# keypoint uncertainty in pixels
# [this can also be considered as a model parameter]
sigma: 8



###########################################
# training parameters
###########################################
--- !train
is_training: true
model_dir: './models'
num_epochs: 0
learning_rate: 0.001
learning_rate_decay:
  decay_steps: 5000
  decay_rate: 0.995

batch_size: 16
shuffle: true

train_keypoints:
  - head
  - left_shoulder
  - right_shoulder
  - left_elbow
  - right_elbow
  - left_wrist
  - nose
  - right_wrist
  - neck
  - left_hip
  - right_hip
  - left_knee
  - right_knee
  - left_ankle
  - right_ankle

train_skeletons:
  - [head, nose]
  - [nose, neck]
  - [neck, left_shoulder]
  - [neck, right_shoulder]
  - [neck, left_hip]
  - [neck, right_hip]
  - [left_shoulder, left_elbow]
  - [right_shoulder, right_elbow]
  - [left_elbow, left_wrist]
  - [right_elbow, right_wrist]
  - [left_hip, left_knee]
  - [right_hip, right_knee]
  - [left_knee, left_ankle]
  - [right_knee, right_ankle]

window_size: 3
vector_scale: 12.
offset_scale: 8.

augmentation:
  flip_left_right: true
  random_crop: true
  random_brightness: true
  random_contrast: true
  scale_range: [2.25, 2.85]
  require_person_in_crop: false

preprocess:
  image_resize: [320, 320]

optimizer:
  name: adam
  params:
    beta1: 0.9
    beta2: 0.999
    epsilon: 1e-8

filenames_shuffle_buffer_size: 100
num_parallel_map_calls: 12  # ~ num of CPU cores or less
num_readers: 32
read_block_length: 64
shuffle_buffer_size: 4096
prefetch_size: 64  # ~ 2 x batch-size

vecmap_loss_weight: 0.1
offsetmap_loss_weight: 1.
bbox_clf_weight: 5.
bbox_reg_weight: 10.

quantize: False

###########################################
# model parameters
###########################################
--- !model
model_name: mobilenet_pose
input_shape: [320, 320]
output_shape: [40, 40]
output_stride: 8

depth_multiplier: 1.
min_depth: 8

# in top-down format
base_anchor_sizes: [60, 120]
anchor_strides: [16, 32]

anchor_scales: [1.]
anchor_ratios: [0.5, 1., 2.]

unmatched_threshold: 0.4
matched_threshold: 0.7
force_match_for_gt_bbox: true
scale_factors: [10., 5.]

backbone_endpoint: layer_17
final_depth: 160


###########################################
# inference parameters
###########################################
--- !infer
model_dir: models/latest
frozen_model: frozen_model.pb
network_input_shape: [320, 320]

# out_stride determines the output shape of network
# For example, if input image is 320 x 320
# With stride = 8, output heatmap is 40 x 40
out_stride: 8

# raw image is downsized to resize_shape [H, W]
# NOTE : Tips for resizing:
# * For speed, downsize image as much as possible
#   but this usually results in loss of accuracy
# * For best speed-accuracy compromise, resize image
#   so that network_input_shape dimension is
#   2 to 4 times bbox dimension
#   where dimension is defined as sqrt(H * W)
# * Try to preserve aspect ratio as far as possible
# Example:
#   [320, 569] for lab videos
#   [456, 800] for Walmart videos
resize_shape: [320, 426]

# overlapping patches are encouraged
# for more robust prediction near edge of patches
# overlap is determined by the strides
# which is the distance between two overlapping patches
# [strides-y, strides-x]
# Example:
#   [1, 249] for lab videos
#   [136, 240] for Walmart videos
strides: [1, 106]

# input_type can be images, video or camera
input_type: camera
# images can be a list or use *
# images: /media/easystore/TrainData/Walmart/Round1/Recording_2/20180308_*.jpg
images: /media/easystore/TrainData/Lab/April20/Recording_44/20180420_*.jpg

video: /media/easystore/TrainData/Lab/April20/Recording_44/20180420_0000000.avi

camera: http://root:123456@192.168.1.13/mjpg/video.mjpg

display_bbox: false

